{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15c2148f-c1b0-46e0-87f6-2db29e13d5b8",
      "metadata": {
        "id": "15c2148f-c1b0-46e0-87f6-2db29e13d5b8"
      },
      "source": [
        "## Masked Autoencoders: Visualization Demo\n",
        "\n",
        "This is a visualization demo using our pre-trained MAE models. No GPU is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fffa39c9-ca9b-4da0-90a4-de96bebbf755",
      "metadata": {
        "id": "fffa39c9-ca9b-4da0-90a4-de96bebbf755"
      },
      "source": [
        "### Prepare\n",
        "Check environment. Install packages if in Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30c12fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('meow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eae7403-f458-4f55-a557-4e045bd6f679",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eae7403-f458-4f55-a557-4e045bd6f679",
        "outputId": "925b0dc2-f037-4e49-c334-d9160a5c0d6a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import requests\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# check whether run in Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install timm==0.4.5  # 0.3.2 does not work in Colab\n",
        "    !git clone https://github.com/facebookresearch/mae.git\n",
        "    sys.path.append('./mae')\n",
        "else:\n",
        "    sys.path.append('..')\n",
        "import models_mae\n",
        "from uncertainty_mae import UncertaintyMAE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f7797ef-412a-439f-911e-3be294047629",
      "metadata": {
        "id": "2f7797ef-412a-439f-911e-3be294047629"
      },
      "source": [
        "### Define utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4573e6be-935a-4106-8c06-e467552b0e3d",
      "metadata": {
        "id": "4573e6be-935a-4106-8c06-e467552b0e3d"
      },
      "outputs": [],
      "source": [
        "# define the utils\n",
        "\n",
        "# imagenet_mean = np.array([0, 0, 0])\n",
        "# imagenet_std = np.array([1, 1, 1])\n",
        "\n",
        "imagenet_mean = 255 * np.array([0.485, 0.456, 0.406])\n",
        "imagenet_std = 255 * np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "def show_image(image, title='', mean=imagenet_mean, std=imagenet_std):\n",
        "    # image is [H, W, 3]\n",
        "    assert image.shape[2] == 3\n",
        "    plt.imshow(torch.clip((image * std + mean), 0, 255).int())\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.axis('off')\n",
        "    return\n",
        "\n",
        "def prepare_model(chkpt_dir, arch='mae_vit_large_patch16'):\n",
        "    # build model\n",
        "    model = models_mae.__dict__[arch](norm_pix_loss=False, \n",
        "                                    quantile=None, \n",
        "                                    vae=True, kld_beta=1)\n",
        "    # load model\n",
        "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
        "    if 'model' in checkpoint:\n",
        "        checkpoint = checkpoint['model']\n",
        "    msg = model.load_state_dict(checkpoint, strict=False)\n",
        "    print(msg)\n",
        "    print('is vae:', model.vae)\n",
        "    return model\n",
        "\n",
        "def prepare_uncertainty_model(chkpt_dir, arch='mae_vit_base_patch16', same_encoder=True):\n",
        "    visible_model = models_mae.__dict__[arch](norm_pix_loss=False, \n",
        "                                    quantile=None, \n",
        "                                    vae=False, kld_beta=0)\n",
        "    invisible_model = models_mae.__dict__[arch](norm_pix_loss=False, \n",
        "                                    quantile=None, \n",
        "                                    vae=True, kld_beta=0, num_vae_blocks=1)\n",
        "    model = UncertaintyMAE(visible_mae=None if same_encoder else visible_model, \n",
        "                           invisible_mae=invisible_model, same_encoder=same_encoder)\n",
        "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
        "    if 'model' in checkpoint:\n",
        "        checkpoint = checkpoint['model']\n",
        "    try:\n",
        "        msg = model.load_state_dict(checkpoint, strict=True)\n",
        "    except RuntimeError as the_error:\n",
        "        print(the_error)\n",
        "        assert 'invisible_mae.logVar_zero_conv_weight' not in checkpoint\n",
        "        assert 'invisible_mae.logVar_zero_conv_bias' not in checkpoint\n",
        "        assert 'invisible_mae.mean_zero_conv_weight' not in checkpoint\n",
        "        assert 'invisible_mae.mean_zero_conv_bias' not in checkpoint\n",
        "\n",
        "        msg = model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "        invisible_mae = model.invisible_mae\n",
        "        invisible_mae.logVar_zero_conv_weight = torch.nn.Parameter(torch.ones(1))\n",
        "        invisible_mae.logVar_zero_conv_bias = torch.nn.Parameter(torch.zeros(0))\n",
        "        invisible_mae.mean_zero_conv_weight = torch.nn.Parameter(torch.ones(1))\n",
        "        invisible_mae.mean_zero_conv_bias = torch.nn.Parameter(torch.zeros(0))\n",
        "\n",
        "    print(msg)\n",
        "\n",
        "    return model\n",
        "\n",
        "def run_one_image(img, model, mask_ratio=0.75, force_mask=None, mean=imagenet_mean, std=imagenet_std):\n",
        "    x = torch.tensor(img)\n",
        "\n",
        "    # make it a batch-like\n",
        "    x = x.unsqueeze(dim=0)\n",
        "    #x = torch.einsum('nhwc->nchw', x)\n",
        "\n",
        "    # run MAE\n",
        "    loss, y, mask = model(x.float(), mask_ratio=mask_ratio, force_mask=force_mask)\n",
        "    if isinstance(model, UncertaintyMAE):\n",
        "        y = model.visible_mae.unpatchify(y)\n",
        "    else:\n",
        "        y = model.unpatchify(y)\n",
        "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
        "\n",
        "    # visualize the mask\n",
        "    mask = mask.detach()\n",
        "    if isinstance(model, UncertaintyMAE):\n",
        "        mask = mask.unsqueeze(-1).repeat(1, 1, model.visible_mae.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
        "        mask = model.visible_mae.unpatchify(mask)  # 1 is removing, 0 is keeping \n",
        "    else:\n",
        "        mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
        "        mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
        "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
        "\n",
        "    x = torch.einsum('nchw->nhwc', x).detach().cpu()\n",
        "\n",
        "    print(x.mean())\n",
        "\n",
        "    # masked image\n",
        "    im_masked = x * (1 - mask)\n",
        "\n",
        "    # MAE reconstruction pasted with visible patches\n",
        "    im_paste = x * (1 - mask) + y * mask\n",
        "\n",
        "    # make the plt figure larger\n",
        "    plt.rcParams['figure.figsize'] = [24, 24]\n",
        "\n",
        "    plt.subplot(1, 4, 1)\n",
        "    show_image(x[0], \"original\", mean=mean, std=std)\n",
        "\n",
        "    plt.subplot(1, 4, 2)\n",
        "    show_image(im_masked[0], \"masked\", mean=mean, std=std)\n",
        "\n",
        "    plt.subplot(1, 4, 3)\n",
        "    show_image(y[0], \"reconstruction\", mean=mean, std=std)\n",
        "\n",
        "    plt.subplot(1, 4, 4)\n",
        "    show_image(im_paste[0], \"reconstruction + visible\", mean=mean, std=std)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a92a06e7-3b6d-4c33-9eb2-15e560a4ce42",
      "metadata": {
        "id": "a92a06e7-3b6d-4c33-9eb2-15e560a4ce42"
      },
      "source": [
        "### Load an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69245e85",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset_generation.emoji_dataset import EmojiDataset\n",
        "from torchvision import datasets, transforms\n",
        "# simple augmentation\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.Resize((224, 224), interpolation=3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "transform_celeba = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((224, 224), scale=(0.6, 1.0), interpolation=3),  # 3 is bicubic\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "# transform_train = transforms.Compose([\n",
        "#         transforms.RandomResizedCrop((224, 224), scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic\n",
        "#         transforms.RandomHorizontalFlip(),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#     ])\n",
        "emnist_mean = np.array([0.176, 0.176, 0.176])\n",
        "emnist_std = np.array([0.328, 0.328, 0.328])\n",
        "emnist_transform = transforms.Compose([\n",
        "        lambda img: transforms.functional.rotate(img, -90),\n",
        "        lambda img: transforms.functional.hflip(img),\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((224, 224), interpolation=3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(emnist_mean, emnist_std)\n",
        "    ])\n",
        "# dataset2 = datasets.CIFAR100('../data', train=False, download=True,\n",
        "#                        transform=transform_test)\n",
        "# dataset2 = datasets.CelebA('/local/zemel/gzg2104/datasets', split='test', target_type='attr', \n",
        "#                            transform=transform_celeba, download=True)\n",
        "dataset2 = datasets.EMNIST('../data', split='balanced', train=False, download=True,\n",
        "                           transform=emnist_transform)\n",
        "test_kwargs = {'batch_size': 1}\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a573ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, img_tuple in enumerate(test_loader):\n",
        "    print(idx)\n",
        "    plt.rcParams['figure.figsize'] = [5, 5]\n",
        "    img, label = img_tuple\n",
        "    print(img.shape)\n",
        "    assert img.shape == (1, 3, 224, 224)\n",
        "    img = img.squeeze()\n",
        "    print(img.mean())\n",
        "    show_image(torch.einsum('chw->hwc', img))#, mean=255*emnist_mean, std=255*emnist_std)\n",
        "    if idx == 11:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa94b2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# sketch_mean = [0.857, 0.857, 0.857]\n",
        "# sketch_std = [0.254, 0.254, 0.254]\n",
        "\n",
        "# transform_test = transforms.Compose([\n",
        "#         transforms.RandomResizedCrop((224, 224), scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic\n",
        "#         transforms.RandomHorizontalFlip(),\n",
        "#         transforms.Grayscale(num_output_channels=3),\n",
        "#         # transforms.Resize((224, 224), interpolation=3),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(sketch_mean, sketch_std)\n",
        "#     ])\n",
        "\n",
        "# def transform_wrapper(examples):\n",
        "#     examples[\"image\"] = [transform_test(image) for image in examples[\"image\"]]\n",
        "#     return examples\n",
        "\n",
        "# dataset = load_dataset(\"imagenet_sketch\", split='train', \n",
        "#                        cache_dir='/local/zemel/gzg2104/datasets')\n",
        "\n",
        "# dataset.set_transform(transform_wrapper)\n",
        "\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# # for idx, item in tqdm(enumerate(dataloader)):\n",
        "# #     print('mean', torch.mean(item['image'], dim=(0, 2, 3)))\n",
        "# #     print('std', torch.std(item['image'], dim=(0, 2, 3)))\n",
        "# #     break\n",
        "\n",
        "# for idx, item in enumerate(dataloader):\n",
        "#     print(item['image'].shape)\n",
        "#     print(item['label'].shape)\n",
        "\n",
        "#     print('mean', torch.mean(item['image'], dim=(0, 2, 3)))\n",
        "#     print('std', torch.std(item['image'], dim=(0, 2, 3)))\n",
        "\n",
        "#     img = item['image'].squeeze()\n",
        "\n",
        "#     show_image(torch.einsum('chw->hwc', img), \n",
        "#                mean = 255 * np.array(sketch_mean), \n",
        "#                std = 255 * np.array(sketch_std))\n",
        "\n",
        "#     if idx == 5:\n",
        "#         break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7e691d-93d2-439f-91d6-c22716a897b5",
      "metadata": {
        "id": "8b7e691d-93d2-439f-91d6-c22716a897b5"
      },
      "source": [
        "### Load a pre-trained MAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff33a44",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thanks ChatGPT!\n",
        "\n",
        "def load_decoder_state_dict(model, chkpt_dir):\n",
        "    state_dict = torch.load(chkpt_dir)['model']\n",
        "    # Filter the state_dict to include only the keys for the desired parameters\n",
        "    filtered_state_dict = {k: v for k, v in state_dict.items() if k.startswith((\n",
        "        'decoder_embed',\n",
        "        'mask_token',\n",
        "        'decoder_pos_embed',\n",
        "        'decoder_blocks',\n",
        "        'decoder_norm',\n",
        "        'decoder_pred'\n",
        "    ))}\n",
        "\n",
        "    # Load the filtered state_dict into the model\n",
        "    # Set strict=False to ignore non-matching keys\n",
        "    model.load_state_dict(filtered_state_dict, strict=False)\n",
        "\n",
        "    print('loaded decoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2d7da9-f75c-4b27-a84b-6d1247f73a7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd2d7da9-f75c-4b27-a84b-6d1247f73a7d",
        "outputId": "5e4a1b84-2d33-4351-cd97-7cd13411b494"
      },
      "outputs": [],
      "source": [
        "# This is an MAE model trained with pixels as targets for visualization (ViT-Large, training mask ratio=0.75)\n",
        "\n",
        "# download checkpoint if not exist\n",
        "!wget -nc https://dl.fbaipublicfiles.com/mae/visualize/mae_visualize_vit_base.pth\n",
        "\n",
        "# chkpt_dir = '/local/zemel/gzg2104/_cifar_models/fromScratch_06_21_24_zeroConv_eps_1e-4/checkpoint-600.pth'\n",
        "# chkpt_dir = '/local/zemel/gzg2104/_cifar_models/06_12_24_batchSize_384/checkpoint-799.pth'\n",
        "# chkpt_dir = '/local/zemel/gzg2104/_cifar_models/REDO_06_12_24_batchSize_384/checkpoint-700.pth'\n",
        "# chkpt_dir = '/local/zemel/gzg2104/_celeba_models/initialTry_06_20_24/checkpoint-200.pth'\n",
        "# chkpt_dir = '/local/zemel/gzg2104/_emnist_models/06_21_24_noZeroConv/checkpoint-40.pth'\n",
        "chkpt_dir = '/local/zemel/gzg2104/_emnist_models/06_24_24/common_encoder/beta5_blr1e-4_eps1e-8/warmup20_total400/checkpoint-80.pth'\n",
        "model_mae = prepare_uncertainty_model(chkpt_dir, 'mae_vit_base_patch16', same_encoder=True)\n",
        "print('Model loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d15a0a7-c093-439a-9a4d-c37ce0c0eaa6",
      "metadata": {
        "id": "7d15a0a7-c093-439a-9a4d-c37ce0c0eaa6"
      },
      "source": [
        "### Run MAE on the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6894df",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(test_loader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "573a12c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def randomize_mask_layout(mask_layout, mask_ratio=0.75):\n",
        "    all_indices = [(i, j) for i in range(mask_layout.shape[0]) for j in range(mask_layout.shape[1])]\n",
        "    random.shuffle(all_indices)\n",
        "    for i, j in all_indices[:int(mask_ratio * len(all_indices))]:\n",
        "        mask_layout[i, j] = 0\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8dac8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_mae = model_mae.cuda()\n",
        "model_mae.eval()\n",
        "random_mask = True\n",
        "    \n",
        "print(model_mae)\n",
        "for idx, img_tuple in enumerate(test_loader):\n",
        "    print(idx)\n",
        "    plt.rcParams['figure.figsize'] = [5, 5]\n",
        "    img, label = img_tuple\n",
        "    print(label)\n",
        "    assert img.shape == (1, 3, 224, 224)\n",
        "    img = img.cuda()\n",
        "    img = img.squeeze()\n",
        "    #show_image(torch.einsum('chw->hwc', img))\n",
        "\n",
        "    torch.manual_seed(idx)\n",
        "    print('MAE with pixel reconstruction:')\n",
        "    mask_layout = torch.ones(14, 14).to(device=img.device)\n",
        "    #print(mask_layout.shape)\n",
        "    if random_mask:\n",
        "        randomize_mask_layout(mask_layout, mask_ratio=0.9)\n",
        "    else:\n",
        "        # mask_layout[0:7, 0:14] = 0\n",
        "        # mask_layout[7:14, 7:14] = 0\n",
        "        mask_layout[0:14, 0:14] = 0\n",
        "    \n",
        "    mask_layout = mask_layout.flatten()\n",
        "    keep_indices = torch.where(mask_layout == 1)[0]\n",
        "    mask_indices = torch.where(mask_layout == 0)[0]\n",
        "    keep_indices = keep_indices.reshape(1, -1)\n",
        "    mask_indices = mask_indices.reshape(1, -1)\n",
        "    ids_shuffle = torch.cat((keep_indices, mask_indices), dim=1)\n",
        "    print('run regular')\n",
        "    mask_ratio = 1 - keep_indices.shape[1] / ids_shuffle.shape[1]\n",
        "    #print('mask ratio:', mask_ratio)\n",
        "    for j in range(3):\n",
        "        print(f'Generate {j}')\n",
        "        # print('mask layout:', mask_layout)\n",
        "        # print('mask layout shape:', mask_layout.shape)\n",
        "        # print('ids shuffle:', ids_shuffle)\n",
        "        if isinstance(model_mae, UncertaintyMAE):\n",
        "            run_one_image(img, model_mae, mask_ratio=mask_ratio, force_mask=(keep_indices, mask_indices),\n",
        "                          mean=255*emnist_mean, std=255*emnist_std)\n",
        "        # else:\n",
        "        #     run_one_image(img, model_mae, mask_ratio=mask_ratio, force_mask=ids_shuffle)\n",
        "    if idx == 20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df2c7e91-3981-44ae-a00e-1b26efa7aa5c",
      "metadata": {
        "id": "df2c7e91-3981-44ae-a00e-1b26efa7aa5c",
        "tags": []
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
