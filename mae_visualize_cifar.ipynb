{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15c2148f-c1b0-46e0-87f6-2db29e13d5b8",
      "metadata": {
        "id": "15c2148f-c1b0-46e0-87f6-2db29e13d5b8"
      },
      "source": [
        "## Masked Autoencoders: Visualization Demo\n",
        "\n",
        "This is a visualization demo using our pre-trained MAE models. No GPU is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fffa39c9-ca9b-4da0-90a4-de96bebbf755",
      "metadata": {
        "id": "fffa39c9-ca9b-4da0-90a4-de96bebbf755"
      },
      "source": [
        "### Prepare\n",
        "Check environment. Install packages if in Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30c12fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('meow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eae7403-f458-4f55-a557-4e045bd6f679",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eae7403-f458-4f55-a557-4e045bd6f679",
        "outputId": "925b0dc2-f037-4e49-c334-d9160a5c0d6a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import requests\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# check whether run in Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install timm==0.4.5  # 0.3.2 does not work in Colab\n",
        "    !git clone https://github.com/facebookresearch/mae.git\n",
        "    sys.path.append('./mae')\n",
        "else:\n",
        "    sys.path.append('..')\n",
        "import models_mae\n",
        "from uncertainty_mae import UncertaintyMAE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f7797ef-412a-439f-911e-3be294047629",
      "metadata": {
        "id": "2f7797ef-412a-439f-911e-3be294047629"
      },
      "source": [
        "### Define utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4573e6be-935a-4106-8c06-e467552b0e3d",
      "metadata": {
        "id": "4573e6be-935a-4106-8c06-e467552b0e3d"
      },
      "outputs": [],
      "source": [
        "# define the utils\n",
        "\n",
        "# imagenet_mean = np.array([0, 0, 0])\n",
        "# imagenet_std = np.array([1, 1, 1])\n",
        "\n",
        "imagenet_mean = 255 * np.array([0.485, 0.456, 0.406])\n",
        "imagenet_std = 255 * np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "def show_image(image, title=''):\n",
        "    # image is [H, W, 3]\n",
        "    assert image.shape[2] == 3\n",
        "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean), 0, 255).int())\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.axis('off')\n",
        "    return\n",
        "\n",
        "def prepare_model(chkpt_dir, arch='mae_vit_large_patch16'):\n",
        "    # build model\n",
        "    model = models_mae.__dict__[arch](norm_pix_loss=False, \n",
        "                                    quantile=None, \n",
        "                                    vae=True, kld_beta=1)\n",
        "    # load model\n",
        "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
        "    if 'model' in checkpoint:\n",
        "        checkpoint = checkpoint['model']\n",
        "    msg = model.load_state_dict(checkpoint, strict=False)\n",
        "    print(msg)\n",
        "    print('is vae:', model.vae)\n",
        "    return model\n",
        "\n",
        "def prepare_uncertainty_model(chkpt_dir, arch='mae_vit_base_patch16'):\n",
        "    visible_model = models_mae.__dict__[arch](norm_pix_loss=False, \n",
        "                                    quantile=None, \n",
        "                                    vae=False, kld_beta=0)\n",
        "    invisible_model = models_mae.__dict__[arch](norm_pix_loss=False, \n",
        "                                    quantile=None, \n",
        "                                    vae=True, kld_beta=0)\n",
        "    model = UncertaintyMAE(visible_mae=visible_model, invisible_mae=invisible_model)\n",
        "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
        "    if 'model' in checkpoint:\n",
        "        checkpoint = checkpoint['model']\n",
        "    msg = model.load_state_dict(checkpoint, strict=True)\n",
        "    print(msg)\n",
        "\n",
        "    return model\n",
        "\n",
        "def run_one_image(img, model, mask_ratio=0.75, force_mask=None):\n",
        "    x = torch.tensor(img)\n",
        "\n",
        "    # make it a batch-like\n",
        "    x = x.unsqueeze(dim=0)\n",
        "    #x = torch.einsum('nhwc->nchw', x)\n",
        "\n",
        "    # run MAE\n",
        "    loss, y, mask = model(x.float(), mask_ratio=mask_ratio, force_mask=force_mask)\n",
        "    if isinstance(model, UncertaintyMAE):\n",
        "        y = model.visible_mae.unpatchify(y)\n",
        "    else:\n",
        "        y = model.unpatchify(y)\n",
        "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
        "\n",
        "    # visualize the mask\n",
        "    mask = mask.detach()\n",
        "    if isinstance(model, UncertaintyMAE):\n",
        "        mask = mask.unsqueeze(-1).repeat(1, 1, model.visible_mae.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
        "        mask = model.visible_mae.unpatchify(mask)  # 1 is removing, 0 is keeping \n",
        "    else:\n",
        "        mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
        "        mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
        "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
        "\n",
        "    x = torch.einsum('nchw->nhwc', x).detach().cpu()\n",
        "\n",
        "    print(x.mean())\n",
        "\n",
        "    # masked image\n",
        "    im_masked = x * (1 - mask)\n",
        "\n",
        "    # MAE reconstruction pasted with visible patches\n",
        "    im_paste = x * (1 - mask) + y * mask\n",
        "\n",
        "    # make the plt figure larger\n",
        "    plt.rcParams['figure.figsize'] = [24, 24]\n",
        "\n",
        "    plt.subplot(1, 4, 1)\n",
        "    show_image(x[0], \"original\")\n",
        "\n",
        "    plt.subplot(1, 4, 2)\n",
        "    show_image(im_masked[0], \"masked\")\n",
        "\n",
        "    plt.subplot(1, 4, 3)\n",
        "    show_image(y[0], \"reconstruction\")\n",
        "\n",
        "    plt.subplot(1, 4, 4)\n",
        "    show_image(im_paste[0], \"reconstruction + visible\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a92a06e7-3b6d-4c33-9eb2-15e560a4ce42",
      "metadata": {
        "id": "a92a06e7-3b6d-4c33-9eb2-15e560a4ce42"
      },
      "source": [
        "### Load an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69245e85",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset_generation.emoji_dataset import EmojiDataset\n",
        "from torchvision import datasets, transforms\n",
        "# simple augmentation\n",
        "transform_test = transforms.Compose([\n",
        "        # lambda img: transforms.functional.rotate(img, -90),\n",
        "        # lambda img: transforms.functional.hflip(img),\n",
        "        # transforms.Grayscale(num_output_channels=3),\n",
        "        #transforms.RandomResizedCrop((224, 224), scale=(0.8, 1), interpolation=3),  # 3 is bicubic\n",
        "        #transforms.RandomHorizontalFlip(),\n",
        "        # transforms.RandomAffine(degrees=(-30, 30), translate=(0.5, 0.5), \n",
        "        #                         scale=(0.8, 1.2), shear=(-15, 15), interpolation=3),\n",
        "        transforms.Resize((224, 224), interpolation=3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "dataset2 = datasets.CIFAR100('../data', train=False, download=True,\n",
        "                       transform=transform_test)\n",
        "# dataset2 = EmojiDataset('/home/gzg2104/uncertainty_mae/dataset_generation/columbia_emoji/val')\n",
        "#                     #     include_keywords=['kiss_', 'couple_with_heart_', 'people_holding_hands_'],\n",
        "#                     #     include_any=True\n",
        "#                     # )\n",
        "test_kwargs = {'batch_size': 1}\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a573ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, img_tuple in enumerate(test_loader):\n",
        "    print(idx)\n",
        "    plt.rcParams['figure.figsize'] = [5, 5]\n",
        "    img, label = img_tuple\n",
        "    print(img.shape)\n",
        "    assert img.shape == (1, 3, 224, 224)\n",
        "    img = img.squeeze()\n",
        "    show_image(torch.einsum('chw->hwc', img))\n",
        "    if idx == 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7e691d-93d2-439f-91d6-c22716a897b5",
      "metadata": {
        "id": "8b7e691d-93d2-439f-91d6-c22716a897b5"
      },
      "source": [
        "### Load a pre-trained MAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff33a44",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thanks ChatGPT!\n",
        "\n",
        "def load_decoder_state_dict(model, chkpt_dir):\n",
        "    state_dict = torch.load(chkpt_dir)['model']\n",
        "    # Filter the state_dict to include only the keys for the desired parameters\n",
        "    filtered_state_dict = {k: v for k, v in state_dict.items() if k.startswith((\n",
        "        'decoder_embed',\n",
        "        'mask_token',\n",
        "        'decoder_pos_embed',\n",
        "        'decoder_blocks',\n",
        "        'decoder_norm',\n",
        "        'decoder_pred'\n",
        "    ))}\n",
        "\n",
        "    # Load the filtered state_dict into the model\n",
        "    # Set strict=False to ignore non-matching keys\n",
        "    model.load_state_dict(filtered_state_dict, strict=False)\n",
        "\n",
        "    print('loaded decoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2d7da9-f75c-4b27-a84b-6d1247f73a7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd2d7da9-f75c-4b27-a84b-6d1247f73a7d",
        "outputId": "5e4a1b84-2d33-4351-cd97-7cd13411b494"
      },
      "outputs": [],
      "source": [
        "# This is an MAE model trained with pixels as targets for visualization (ViT-Large, training mask ratio=0.75)\n",
        "\n",
        "# download checkpoint if not exist\n",
        "!wget -nc https://dl.fbaipublicfiles.com/mae/visualize/mae_visualize_vit_base.pth\n",
        "\n",
        "chkpt_dir = '/local/zemel/gzg2104/cifar_train_partial_vae_base_vit_batch_256_beta_5_epochs_800_decay_0_05/checkpoint-799.pth'\n",
        "# chkpt_dir = '/local/zemel/gzg2104/emoji_train_partial_vae_base_vit_batch_256_beta_5/checkpoint-3999.pth'\n",
        "model_mae = prepare_uncertainty_model(chkpt_dir, 'mae_vit_base_patch16')\n",
        "print('Model loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d15a0a7-c093-439a-9a4d-c37ce0c0eaa6",
      "metadata": {
        "id": "7d15a0a7-c093-439a-9a4d-c37ce0c0eaa6"
      },
      "source": [
        "### Run MAE on the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6894df",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(test_loader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "573a12c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def randomize_mask_layout(mask_layout, mask_ratio=0.75):\n",
        "    all_indices = [(i, j) for i in range(mask_layout.shape[0]) for j in range(mask_layout.shape[1])]\n",
        "    random.shuffle(all_indices)\n",
        "    for i, j in all_indices[:int(mask_ratio * len(all_indices))]:\n",
        "        mask_layout[i, j] = 0\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8dac8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_mae = model_mae.cuda()\n",
        "model_mae.eval()\n",
        "random_mask = True\n",
        "    \n",
        "print(model_mae)\n",
        "for idx, img_tuple in enumerate(test_loader):\n",
        "    print(idx)\n",
        "    plt.rcParams['figure.figsize'] = [5, 5]\n",
        "    img, label = img_tuple\n",
        "    assert img.shape == (1, 3, 224, 224)\n",
        "    img = img.cuda()\n",
        "    img = img.squeeze()\n",
        "    #show_image(torch.einsum('chw->hwc', img))\n",
        "\n",
        "    torch.manual_seed(idx)\n",
        "    print('MAE with pixel reconstruction:')\n",
        "    mask_layout = torch.ones(14, 14).to(device=img.device)\n",
        "    #print(mask_layout.shape)\n",
        "    if random_mask:\n",
        "        randomize_mask_layout(mask_layout, mask_ratio=0.75)\n",
        "    else:\n",
        "        mask_layout[0:14, 0:7] = 0\n",
        "    \n",
        "    mask_layout = mask_layout.flatten()\n",
        "    keep_indices = torch.where(mask_layout == 1)[0]\n",
        "    mask_indices = torch.where(mask_layout == 0)[0]\n",
        "    keep_indices = keep_indices.reshape(1, -1)\n",
        "    mask_indices = mask_indices.reshape(1, -1)\n",
        "    ids_shuffle = torch.cat((keep_indices, mask_indices), dim=1)\n",
        "    print('run regular')\n",
        "    mask_ratio = 1 - keep_indices.shape[1] / ids_shuffle.shape[1]\n",
        "    #print('mask ratio:', mask_ratio)\n",
        "    for j in range(3):\n",
        "        print(f'Generate {j}')\n",
        "        # print('mask layout:', mask_layout)\n",
        "        # print('mask layout shape:', mask_layout.shape)\n",
        "        # print('ids shuffle:', ids_shuffle)\n",
        "        if isinstance(model_mae, UncertaintyMAE):\n",
        "            run_one_image(img, model_mae, mask_ratio=mask_ratio, force_mask=(keep_indices, mask_indices))\n",
        "        else:\n",
        "            run_one_image(img, model_mae, mask_ratio=mask_ratio, force_mask=ids_shuffle)\n",
        "    if idx == 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df2c7e91-3981-44ae-a00e-1b26efa7aa5c",
      "metadata": {
        "id": "df2c7e91-3981-44ae-a00e-1b26efa7aa5c",
        "tags": []
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
